{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "> This module contains a PyTorch implementation of the Deep Recurrent Survival Analysis model, which is trained on sequence-to-sequence data with binary labels at each time step, where the event always occurs at the final time step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "import pytest\n",
    "import torch.optim as optim\n",
    "from drsa.functions import event_time_loss, event_rate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class DRSA(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Recurrent Survival Analysis model.\n",
    "    A relatively shallow net, characterized by an LSTM layer followed by a Linear layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int,\n",
    "        hidden_dim: int,\n",
    "        n_layers: int,\n",
    "        embeddings: List[nn.Embedding],\n",
    "        output_size: int = 1,\n",
    "        LSTM_dropout: float = 0.0,\n",
    "        Linear_dropout: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        * `n_features`\n",
    "            - size of the input to the LSTM (number of features)\n",
    "        * `hidden_dim`:\n",
    "            - size (dimension) of the hidden state in LSTM\n",
    "        * `n_layers`:\n",
    "            - number of layers in LSTM\n",
    "        * `embeddings`:\n",
    "            - list of nn.Embeddings for each categorical variable\n",
    "            - It is assumed the the 1st categorical feature corresponds with the 0th feature,\n",
    "              the 2nd corresponds with the 1st feature, and so on.\n",
    "        * `output_size`:\n",
    "            - size of the linear layer's output, which should always be 1, unless altering this model\n",
    "        * `LSTM_dropout`:\n",
    "            - percent of neurons in LSTM layer to apply dropout regularization to during training\n",
    "        * `Linear_dropout`:\n",
    "            - percent of neurons in linear layer to apply dropout regularization to during training\n",
    "        \"\"\"\n",
    "        super(DRSA, self).__init__()\n",
    "\n",
    "        # hyper params\n",
    "        self.n_features = n_features\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # embeddings\n",
    "        self.embeddings = embeddings\n",
    "        \n",
    "\n",
    "        # model architecture\n",
    "        self.lstm = nn.LSTM(\n",
    "            sum([emb.embedding_dim for emb in self.embeddings])\n",
    "            + self.n_features\n",
    "            - len(self.embeddings),\n",
    "            self.hidden_dim,\n",
    "            self.n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=LSTM_dropout,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.linear_dropout = nn.Dropout(p=Linear_dropout)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # making sure all params get trained\n",
    "        self.params_to_train = nn.ModuleList(self.embeddings + [self.lstm, self.fc])\n",
    "\n",
    "    def forward(self, X: torch.tensor):\n",
    "        \"\"\"\n",
    "        input:\n",
    "        * `X`\n",
    "            - input features of shape (batch_size, sequence length, self.n_features)\n",
    "            \n",
    "        output:\n",
    "        * `out`: \n",
    "            - the DRSA model's predictions at each time step, for each observation in batch\n",
    "            - out is of shape (batch_size, sequence_length, 1)\n",
    "        \"\"\"\n",
    "        # concatenating embedding and numeric features\n",
    "        all_embeddings = [\n",
    "            emb(X[:, :, i].long()) for i, emb in enumerate(self.embeddings)\n",
    "        ]\n",
    "        other_features = X[:, :, len(self.embeddings) :]\n",
    "        all_features = torch.cat(all_embeddings + [other_features.float()], dim=-1)\n",
    "\n",
    "        # passing input and hidden into model (hidden initialized as zeros)\n",
    "        out, hidden = self.lstm(all_features.float())\n",
    "\n",
    "        # passing to linear layer to reshape for predictions\n",
    "        out = self.sigmoid(self.linear_dropout(self.fc(out)))\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"DRSA.__init__\" class=\"doc_header\"><code>DRSA.__init__</code><a href=\"__main__.py#L9\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>DRSA.__init__</code>(**`n_features`**:`int`, **`hidden_dim`**:`int`, **`n_layers`**:`int`, **`embeddings`**:`List`\\[`Embedding`\\], **`output_size`**:`int`=*`1`*, **`LSTM_dropout`**:`float`=*`0.0`*, **`Linear_dropout`**:`float`=*`0.0`*)\n",
       "\n",
       "inputs:\n",
       "* `n_features`\n",
       "    - size of the input to the LSTM (number of features)\n",
       "* `hidden_dim`:\n",
       "    - size (dimension) of the hidden state in LSTM\n",
       "* `n_layers`:\n",
       "    - number of layers in LSTM\n",
       "* `embeddings`:\n",
       "    - list of nn.Embeddings for each categorical variable\n",
       "    - It is assumed the the 1st categorical feature corresponds with the 0th feature,\n",
       "      the 2nd corresponds with the 1st feature, and so on.\n",
       "* `output_size`:\n",
       "    - size of the linear layer's output, which should always be 1, unless altering this model\n",
       "* `LSTM_dropout`:\n",
       "    - percent of neurons in LSTM layer to apply dropout regularization to during training\n",
       "* `Linear_dropout`:\n",
       "    - percent of neurons in linear layer to apply dropout regularization to during training"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"DRSA.forward\" class=\"doc_header\"><code>DRSA.forward</code><a href=\"__main__.py#L66\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>DRSA.forward</code>(**`X`**:`tensor`)\n",
       "\n",
       "input:\n",
       "* `X`\n",
       "    - input features of shape (batch_size, sequence length, self.n_features)\n",
       "output:\n",
       "* `out`: \n",
       "    - the DRSA model's predictions at each time step, for each observation in batch\n",
       "    - out is of shape (batch_size, sequence_length, 1)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(DRSA.__init__)\n",
    "show_doc(DRSA.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "# generating random data\n",
    "batch_size, seq_len, n_features = (64, 25, 10)\n",
    "data = torch.randn(batch_size, seq_len, n_features)\n",
    "\n",
    "# generating random embedding for each sequence\n",
    "n_embeddings = 10\n",
    "embedding_idx = torch.mul(\n",
    "    torch.ones(batch_size, seq_len, 1),\n",
    "    torch.randint(low=0, high=n_embeddings, size=(batch_size, 1, 1)),\n",
    ")\n",
    "\n",
    "# concatenating embeddings and features\n",
    "X = torch.cat([embedding_idx, data], dim=-1)\n",
    "\n",
    "# instantiating embedding parameters\n",
    "embedding_size = 5\n",
    "embeddings = torch.nn.Embedding(n_embeddings, embedding_size)\n",
    "\n",
    "# instantiating model\n",
    "model = DRSA(\n",
    "    n_features=n_features + 1,  # +1 for the embeddings\n",
    "    hidden_dim=2,\n",
    "    n_layers=1,\n",
    "    embeddings=[embeddings],\n",
    ")\n",
    "\n",
    "\n",
    "# defining training loop\n",
    "def training_loop(X, optimizer, alpha, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X)\n",
    "\n",
    "        # weighted average of survival analysis losses\n",
    "        evt_loss = event_time_loss(preds)\n",
    "        evr_loss = event_rate_loss(preds)\n",
    "        loss = (alpha * evt_loss) + ((1 - alpha) * evr_loss)\n",
    "\n",
    "        # updating parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"epoch: {epoch} - loss: {round(loss.item(), 4)}\")\n",
    "            \n",
    "# running training loop\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "training_loop(X, optimizer, alpha=0.5, epochs=101)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
